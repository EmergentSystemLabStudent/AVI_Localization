{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep Markov Model for full localization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "import torch\n",
    "from torch import optim\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torchvision import transforms, datasets\n",
    "from tensorboardX import SummaryWriter\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 128#128\n",
    "epochs = 600\n",
    "seed = 1\n",
    "torch.manual_seed(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if torch.cuda.is_available():\n",
    "    device = \"cuda\"\n",
    "else:\n",
    "    device = \"cpu\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#計測モデルとか\n",
    "def get_ot(st,lmap,max_range):\n",
    "    dis = torch.sqrt((st[:,0]-lmap[0])**2+(st[:,1]-lmap[1])**2)\n",
    "    angle = torch.atan2((lmap[1]-st[:,1]),(lmap[0]-st[:,0]))-st[:,2]\n",
    "    return torch.stack([dis,angle],1)\n",
    "    \n",
    "def get_all_ot(st,lmap,max_range):\n",
    "    measure = get_ot(st,lmap[0],max_range)\n",
    "    for l in range(1,len(lmap)):\n",
    "        measure = torch.cat([measure, get_ot(st,lmap[l],max_range)],1)\n",
    "    return torch.tensor(measure)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "landmark_num = 10\n",
    "start_pos = [2.0,4.0,0.0]#x0,y0,yaw0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "landmark_dim = 2\n",
    "x_dim = landmark_num*2\n",
    "h_dim = 32 #32\n",
    "hidden_dim = 32 #32\n",
    "z_dim = 3\n",
    "u_dim = 2\n",
    "t_max = 139"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#データの読み込み\n",
    "transform = transforms.Compose([transforms.ToTensor()])\n",
    "kwargs = {'batch_size': batch_size, 'num_workers': 1, 'pin_memory': True}\n",
    "\n",
    "#data loader #とりあえず1時系列分を分身させて食わせてる\n",
    "#[time,s_x,s_y,s_yaw,uv,ur,ot[1],,,,ot[N]]\n",
    "data = np.loadtxt('vehicle_motion_data.csv', delimiter=',')\n",
    "data = torch.tensor([data],dtype=torch.float32)\n",
    "st = data[0,:,1:(1+z_dim)]\n",
    "ut = data[0,:,(1+z_dim):(1+z_dim+u_dim)]\n",
    "ot = data[0,:,(1+z_dim+u_dim):(1+z_dim+u_dim+x_dim)]\n",
    "t_max = len(ot)\n",
    "\n",
    "print(st.size())\n",
    "st=st.repeat(1000,1,1)\n",
    "ut=ut.repeat(1000,1,1)\n",
    "ot=ot.repeat(1000,1,1)\n",
    "print(st.size())\n",
    "\n",
    "\n",
    "landmark = np.loadtxt('landmark_data.csv',delimiter=',')\n",
    "\n",
    "train = torch.utils.data.TensorDataset(ot,ut)\n",
    "train_loader = torch.utils.data.DataLoader(train, shuffle=False,**kwargs)\n",
    "test = torch.utils.data.TensorDataset(ot,ut)\n",
    "test_loader = torch.utils.data.DataLoader(test, shuffle=False,**kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pixyz.models import Model\n",
    "from pixyz.losses import KullbackLeibler, CrossEntropy, IterativeLoss\n",
    "from pixyz.distributions import Bernoulli, Normal, Deterministic\n",
    "from pixyz.utils import print_latex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNN(Deterministic):\n",
    "    def __init__(self):\n",
    "        super(RNN, self).__init__(cond_var=[\"x\"], var=[\"h\"])\n",
    "        self.rnn = nn.GRU(x_dim, h_dim, bidirectional=True)\n",
    "#         self.h0 = torch.zeros(2, batch_size, self.rnn.hidden_size).to(device)\n",
    "        self.h0 = nn.Parameter(torch.zeros(2, 1, self.rnn.hidden_size))\n",
    "        self.hidden_size = self.rnn.hidden_size\n",
    "        \n",
    "    def forward(self, x):\n",
    "        h0 = self.h0.expand(2, x.size(1), self.rnn.hidden_size).contiguous()\n",
    "        h, _ = self.rnn(x, h0)\n",
    "        return {\"h\": h}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class Generator(Bernoulli):\n",
    "#     def __init__(self):\n",
    "#         super(Generator, self).__init__(cond_var=[\"z\"], var=[\"x\"])\n",
    "#         self.fc1 = nn.Linear(z_dim, hidden_dim)\n",
    "#         self.fc2 = nn.Linear(hidden_dim, x_dim)\n",
    "    \n",
    "#     def forward(self, z):\n",
    "#         print(z.size()) #[128,3]\n",
    "#         h = F.relu(self.fc1(z))\n",
    "#         return {\"probs\": torch.sigmoid(self.fc2(h))}\n",
    "class Generator(Normal):\n",
    "    def __init__(self):\n",
    "        super(Generator, self).__init__(cond_var=[\"z\"], var=[\"x\"])\n",
    "    \n",
    "    def forward(self, z):#計測モデルそのまま\n",
    "        ot=get_all_ot(z,landmark,[1000,1000])\n",
    "        return {\"loc\": ot,\"scale\":torch.tensor(0.3).to(device)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Inference(Normal):\n",
    "    def __init__(self):\n",
    "        super(Inference, self).__init__(cond_var=[\"h\", \"z_prev\", \"u\"], var=[\"z\"])\n",
    "        self.fc1 = nn.Linear(z_dim, h_dim*2)\n",
    "        self.fc2 = nn.Linear(u_dim, h_dim*2)\n",
    "        self.fc31 = nn.Linear(h_dim*2, z_dim)\n",
    "        self.fc32 = nn.Linear(h_dim*2, z_dim)\n",
    "        \n",
    "    def forward(self, h, z_prev, u):\n",
    "        h_z = torch.tanh(self.fc1(z_prev))\n",
    "        h_u = torch.tanh(self.fc2(u))\n",
    "        h = (1.0/3.0) * (h + h_z + h_u)\n",
    "        return {\"loc\": self.fc31(h), \"scale\": F.softplus(self.fc32(h))}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Prior(Normal):\n",
    "    def __init__(self):\n",
    "        super(Prior, self).__init__(cond_var=[\"z_prev\",\"u\"], var=[\"z\"])\n",
    "        \n",
    "    def forward(self, z_prev, u):\n",
    "        # motion model for two-wheel robot x,y,orient,v,steering\n",
    "        z = torch.zeros(len(z_prev),z_dim).to(device)\n",
    "        z[:,2] = z_prev[:,2] + u[:,1]\n",
    "        z[:,0] = z_prev[:,0] + u[:,0] * torch.cos(z_prev[:,2] + u[:,1])\n",
    "        z[:,1] = z_prev[:,1] + u[:,0] * torch.sin(z_prev[:,2] + u[:,1])\n",
    "\n",
    "        return {\"loc\": z, \"scale\": torch.tensor(0.3).to(device)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prior = Prior().to(device)\n",
    "encoder = Inference().to(device)\n",
    "decoder = Generator().to(device)\n",
    "rnn = RNN().to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(prior)\n",
    "print(\"*\"*80)\n",
    "print(encoder)\n",
    "print(\"*\"*80)\n",
    "print(decoder)\n",
    "print(\"*\"*80)\n",
    "print(rnn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generate_from_prior = prior * decoder\n",
    "print(generate_from_prior)\n",
    "print_latex(generate_from_prior)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "step_loss = CrossEntropy(encoder, decoder) + KullbackLeibler(encoder, prior)\n",
    "_loss = IterativeLoss(step_loss, max_iter=t_max, \n",
    "                      series_var=[\"x\", \"h\", \"u\"], update_value={\"z\": \"z_prev\"})\n",
    "loss = _loss.expectation(rnn).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dmm = Model(loss, distributions=[rnn, encoder, decoder, prior], \n",
    "            optimizer=optim.RMSprop, optimizer_params={\"lr\": 5e-4}, clip_grad_value=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(dmm)\n",
    "print_latex(dmm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_loop(epoch, loader, model, device, train_mode=False):\n",
    "    mean_loss = 0\n",
    "    for idx,[o,u] in enumerate(tqdm(loader)):#batch_idx, (data, _) in enumerate(tqdm(loader)):\n",
    "        o = o.to(device)\n",
    "        u = u.to(device)\n",
    "        batch_size = o.size()[0]\n",
    "        x = o.transpose(0, 1) #多分転置してるだけ\n",
    "        u = u.transpose(0, 1)\n",
    "        z_prev = torch.tensor(start_pos)#初期姿勢\n",
    "        z_prev = z_prev.repeat(batch_size, 1).to(device)\n",
    "        if train_mode:\n",
    "            mean_loss += model.train({'x': x, 'u':u, 'z_prev': z_prev}).item() * batch_size\n",
    "        else:\n",
    "            mean_loss += model.test({'x': x, 'u':u, 'z_prev': z_prev}).item() * batch_size\n",
    "    mean_loss /= len(loader.dataset)\n",
    "    if train_mode:\n",
    "        print('Epoch: {} Train loss: {:.4f}'.format(epoch, mean_loss))\n",
    "    else:\n",
    "        print('Test loss: {:.4f}'.format(mean_loss))\n",
    "    return mean_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_image_from_latent(batch_size):\n",
    "    x = []\n",
    "    z_prev = torch.zeros(batch_size, z_dim).to(device)\n",
    "    u0 = torch.zeros(batch_size, u_dim).to(device)\n",
    "    for step in range(t_max):\n",
    "        samples = generate_from_prior.sample({'z_prev': z_prev,'u':u0})\n",
    "        x_t = decoder.sample_mean({\"z\": samples[\"z\"]})\n",
    "        z_prev = samples[\"z\"]\n",
    "        x.append(x_t[None, :])\n",
    "    x = torch.cat(x, dim=0).transpose(0, 1)\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "writer = SummaryWriter()\n",
    "\n",
    "history = {\"train_loss\":[],\"test_loss\":[]}\n",
    "\n",
    "for epoch in range(1, epochs + 1):\n",
    "    train_loss = data_loop(epoch, train_loader, dmm, device, train_mode=True)\n",
    "    test_loss = data_loop(epoch, test_loader, dmm, device)\n",
    "\n",
    "    writer.add_scalar('train_loss', train_loss, epoch)\n",
    "    writer.add_scalar('test_loss', test_loss, epoch)\n",
    "\n",
    "    #loss描画用\n",
    "    history[\"train_loss\"].append(train_loss)\n",
    "    history[\"test_loss\"].append(test_loss)\n",
    "    \n",
    "    sample = plot_image_from_latent(batch_size)[:, None][1,:]\n",
    "    writer.add_image('Image_from_latent', sample, epoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "#Loss plot\n",
    "plt.ylabel('$loss$', fontsize=16)\n",
    "plt.xlabel('$epoch$', fontsize=16)\n",
    "ay=plt.gca()\n",
    "plt.title(\"train_loss\")\n",
    "plt.plot(range(epochs), [i+0.5 for i in history[\"train_loss\"]])\n",
    "plt.show()\n",
    "ay=plt.gca()\n",
    "plt.title(\"test_loss\")\n",
    "plt.plot(range(epochs), [i+0.4 for i in history[\"test_loss\"]])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "inference_net = rnn*encoder\n",
    "test_o = data[0,:,(1+z_dim+u_dim):(1+z_dim+u_dim+x_dim)]\n",
    "test_o = torch.tensor(test_o).reshape(1,len(test_o),x_dim).to(device)\n",
    "test_u = data[0,:,(1+z_dim):(1+z_dim+u_dim)]\n",
    "test_u = torch.tensor(test_u).reshape(1,len(test_u),u_dim).to(device)\n",
    "z_prev = torch.tensor(start_pos).to(device)\n",
    "infered_result = inference_net.sample({\"x\":test_o,\"z_prev\":z_prev,\"u\":test_u})[\"z\"].to(\"cpu\")\n",
    "infered_result=infered_result.numpy()\n",
    "\n",
    "plt.plot(infered_result[:,:, 0], infered_result[:,:, 1], \"co\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
